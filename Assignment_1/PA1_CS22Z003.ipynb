{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " PA1 CS22Z003.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPQRJ2uV55B7+ReevdfCO9L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akansh12/Deep-Learning-for-imaging-EE5179/blob/main/Assignment_1/PA1_CS22Z003.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST Classification using MLP\n"
      ],
      "metadata": {
        "id": "q-Acap1SgWyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By \n",
        "- Akansh Maurya (CS22Z003)"
      ],
      "metadata": {
        "id": "2RgCHb3Ui82c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tasks to to:\n",
        "- Baseline: IP-h(500)-h(250)-h(100)-OP\n",
        "- Activation functions for hidden layers: Sigmoid and for the Output layer it is softmax\n",
        "- Gradient-Descent\n",
        "- lr = 0.01, batch size = 64, epoch = 15\n",
        "-  Extra marks for experimentations\n",
        "- Glorot Initialization\n",
        "- Plot the loss for 200 iterations\n",
        "- Confustion matrix and classification report\n",
        "1.1\n",
        "- Use Tanh and RELU\n",
        "\n",
        "Task 2:\n",
        "- Use pytorch for the same\n",
        "- Add L2 Regularization to it."
      ],
      "metadata": {
        "id": "bi2x5f2qgSUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Imports"
      ],
      "metadata": {
        "id": "rSZ1i9nPjYDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "import keras\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from  matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "CNpq2VWIjdBS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the Dataset"
      ],
      "metadata": {
        "id": "MtymctNZjUlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# source: https://keras.io/api/datasets/mnist/\n",
        "tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuSuNiYjgTBx",
        "outputId": "76252a2c-e02e-4f68-94f4-7e67679ea5a1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(10000, 28, 28)\n",
            "(10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reshaping the dataset"
      ],
      "metadata": {
        "id": "ekNbyGONng-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.array(x_train.reshape(x_train.shape[0], 784)).T\n",
        "x_test = np.array(x_test.reshape(x_test.shape[0], 784)).T\n",
        "x_train = (x_train/255.0).astype(np.float32)\n",
        "x_test = (x_test/255.0).astype(np.float32)"
      ],
      "metadata": {
        "id": "Z1Rh7u8GnRBG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train Dataset Shape: \", x_train.shape)\n",
        "print(\"Train Target Vector Shape: \", y_train.shape) \n",
        "print(\"Test Dataset Shape:\", x_test.shape)\n",
        "print(\"Test Target Vector Shape\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Igf2TjDSkyP6",
        "outputId": "e864943b-760c-4d94-f9c1-f8d20731f655"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset Shape:  (784, 60000)\n",
            "Train Target Vector Shape:  (60000,)\n",
            "Test Dataset Shape: (784, 10000)\n",
            "Test Target Vector Shape (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.imshow(x_train[:,48].reshape(28,28))\n",
        "print(y_train[48])\n",
        "plt.figure()\n",
        "plt.imshow(x_test[:,75].reshape(28,28))\n",
        "print(y_test[75])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "id": "g4GmV_evncOl",
        "outputId": "086b6abd-f6ce-4d6b-fc0f-80ea901a92f0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANj0lEQVR4nO3de4xc9XnG8efBXmxj4+DFYFzjlkuMIlopTroxbUAVyHLiuH+YqBKK1UZUQl2amgraFJUmlUL/KkoToqolkQxYOFFKSsrNkVASx0WitJXLgowvOImB2MGOLwUXbNLEl+XtH3ucbuyd3yxz5ma/3480mpnznjPn9cjPnjPnzJyfI0IAzn7n9LoBAN1B2IEkCDuQBGEHkiDsQBJTu7mycz0tpmtmN1cJpPJz/VTH4qgnqtUKu+3lkv5e0hRJD0TEPaX5p2umrvHSOqsEULApNjastbwbb3uKpPskfUzS1ZJW2b661dcD0Fl1PrMvkfRyRLwaEcckfUPSyva0BaDd6oR9gaTXxj3fU037JbaHbY/YHjmuozVWB6COjh+Nj4g1ETEUEUMDmtbp1QFooE7Y90paOO75pdU0AH2oTtifk7TI9uW2z5X0CUnr29MWgHZr+dRbRJywfZuk72js1NvaiNjets4AtFWt8+wR8ZSkp9rUC4AO4uuyQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSRqDdlse5ekI5JGJZ2IiKF2NAWg/WqFvXJDRLzehtcB0EHsxgNJ1A17SPqu7edtD080g+1h2yO2R47raM3VAWhV3d346yJir+2LJW2w/f2IeGb8DBGxRtIaSZrtwai5PgAtqrVlj4i91f1BSY9LWtKOpgC0X8thtz3T9vknH0v6iKRt7WoMQHvV2Y2fJ+lx2ydf558i4ttt6QpA27Uc9oh4VdL729gLgA7i1BuQBGEHkiDsQBKEHUiCsANJtOOHMEBL/Ju/Xqzf8c//UqzvOja3WP/WssYni07s/Ulx2bMRW3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSCLNefYpcy8s1j37/NZf/OixYrnT53SnXPCehjUPzikuG28dLtb9ntkt9XTSsYWN17/6gW8Wl10242fF+hvTflisr7/gusbFvcVFz0ps2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgiTTn2b//hcuK9Z3L7m/5tf/9aPlv5mf/4tZifc+y8kA50+eWzzevuGJ7w9rnL3m6uOy9/7OoWP/zOTuL9V768L+tLtav3L65S52cGdiyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASac6zT/3JucX6l9+8vFifN/Bmw9qOny0oLvvHf1u+/vnCgTeK9WunvVOs13Hn4CvF+mj5KwAdtXLn7xbrV/1Z+ToBo+1s5izQdMtue63tg7a3jZs2aHuD7Z3VffkKCQB6bjK78Q9JWn7KtLskbYyIRZI2Vs8B9LGmYY+IZyQdOmXySknrqsfrJN3Y5r4AtFmrn9nnRcS+6vF+SfMazWh7WNKwJE3XeS2uDkBdtY/GR0RIangYJyLWRMRQRAwNaFrd1QFoUathP2B7viRV9wfb1xKATmg17Osl3Vw9vlnSk+1pB0CneGwvvDCD/bCk6yXNlXRA0uckPSHpEUm/Kmm3pJsi4tSDeKeZ7cG4xktrttwZpWuvS5LnXNCwduJHu4vLTl14abE+enF53W9dVeOa9h02/1Pl8/TfvPI7DWuvnCj/Tn/175d/r37Os/xe/VSbYqMOxyFPVGt6gC4iVjUo9WdqAUyIr8sCSRB2IAnCDiRB2IEkCDuQRJqfuDYz+uZb5Rma1QtOvLanPEOT+uznW151beecV/6K8zl/0vpXoH/vvjuL9V959j9afm2cji07kARhB5Ig7EAShB1IgrADSRB2IAnCDiTBeXYU7fyb9xfrP7jivpZfe9ZrnbtENk7Hlh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkuA8O4reuehYreW3HjvesHb+j39e67Xx7rBlB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkOM9+Bth/x4eL9WOzO7fuG963pdby7x1oPCT4zlvK//2mLy3/u5uZteT1hrUZDzQegluSZjzxX7XW3Y+abtltr7V90Pa2cdPutr3X9ubqtqKzbQKoazK78Q9JWj7B9C9FxOLq9lR72wLQbk3DHhHPSDrUhV4AdFCdA3S32d5S7ebPaTST7WHbI7ZHjutojdUBqKPVsH9F0pWSFkvaJ+mLjWaMiDURMRQRQwOa1uLqANTVUtgj4kBEjEbEO5Lul7SkvW0BaLeWwm57/rinH5e0rdG8APpD0/Psth+WdL2kubb3SPqcpOttL5YUknZJurWDPZ713rjlt4v1kTv/oVg/R2553VNc/ns/GvWu7T7D5zasvfzRNbVeu44PXbK6WJ/RpT66qWnYI2LVBJMf7EAvADqIr8sCSRB2IAnCDiRB2IEkCDuQBD9x7QMXPvifxfrii/+0WF/zR//YsLZw6v8Wlx0oVqX5U2c1maPs9dGfNqwN/+jG4rIv/vjSYn3wX6eX62sbv68Xqfyen43YsgNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEo5ofKnfdpvtwbjGS7u2PjQ35cLBYv1bW75XrB+NE8X6DX91e8PaBV/Ld6670zbFRh2OQxP+5pktO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwe/ZUcsjb5d/c8659P7Blh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkuA8e3J7/+B9TeYo/54dZ46mW3bbC20/bfsl29tt315NH7S9wfbO6n5O59sF0KrJ7MafkPTpiLha0m9JWm37akl3SdoYEYskbayeA+hTTcMeEfsi4oXq8RFJOyQtkLRS0rpqtnWSymP5AOipd/WZ3fZlkj4gaZOkeRGxryrtlzSvwTLDkoYlabrOa7VPADVN+mi87VmSHpV0R0QcHl+LsatWTnjlyohYExFDETE0oGm1mgXQukmF3faAxoL+9Yh4rJp8wPb8qj5f0sHOtAigHZruxtu2pAcl7YiIe8eV1ku6WdI91f2THekQHXXkqtFet4Aumcxn9mslfVLSVtubq2mf0VjIH7F9i6Tdkm7qTIsA2qFp2CPiWUkTXnReEiM+AGcIvi4LJEHYgSQIO5AEYQeSIOxAEvzENbkvf/ShWss/duCDTebYX+v10T5s2YEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCc6zJ7f8vKPF+uiE1x/6f7ufuKJYv4Tz7H2DLTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4k0TTsthfaftr2S7a32769mn637b22N1e3FZ1vF0CrJnPxihOSPh0RL9g+X9LztjdUtS9FxBc61x6AdpnM+Oz7JO2rHh+xvUPSgk43BqC93tVndtuXSfqApE3VpNtsb7G91vacBssM2x6xPXJc5UsgAeicSYfd9ixJj0q6IyIOS/qKpCslLdbYlv+LEy0XEWsiYigihgY0rQ0tA2jFpMJue0BjQf96RDwmSRFxICJGI+IdSfdLWtK5NgHUNZmj8Zb0oKQdEXHvuOnzx832cUnb2t8egHaZzNH4ayV9UtJW25uraZ+RtMr2YkkhaZekWzvSITrqQ3/9qWL90bv/rlif+yLHYc4Ukzka/6wkT1B6qv3tAOgUvkEHJEHYgSQIO5AEYQeSIOxAEoQdSMIRTcbkbaPZHoxrvLRr6wOy2RQbdTgOTXSqnC07kAVhB5Ig7EAShB1IgrADSRB2IAnCDiTR1fPstv9b0u5xk+ZKer1rDbw7/dpbv/Yl0Vur2tnbr0XERRMVuhr201Zuj0TEUM8aKOjX3vq1L4neWtWt3tiNB5Ig7EASvQ77mh6vv6Rfe+vXviR6a1VXeuvpZ3YA3dPrLTuALiHsQBI9Cbvt5bZ/YPtl23f1oodGbO+yvbUahnqkx72stX3Q9rZx0wZtb7C9s7qfcIy9HvXWF8N4F4YZ7+l71+vhz7v+md32FEk/lLRM0h5Jz0laFREvdbWRBmzvkjQUET3/Aobt35H0tqSvRsRvVNM+L+lQRNxT/aGcExF/2Se93S3p7V4P412NVjR//DDjkm6U9Ifq4XtX6OsmdeF968WWfYmklyPi1Yg4Jukbklb2oI++FxHPSDp0yuSVktZVj9dp7D9L1zXorS9ExL6IeKF6fETSyWHGe/reFfrqil6EfYGk18Y936P+Gu89JH3X9vO2h3vdzATmRcS+6vF+SfN62cwEmg7j3U2nDDPeN+9dK8Of18UButNdFxEflPQxSaur3dW+FGOfwfrp3OmkhvHulgmGGf+FXr53rQ5/Xlcvwr5X0sJxzy+tpvWFiNhb3R+U9Lj6byjqAydH0K3uD/a4n1/op2G8JxpmXH3w3vVy+PNehP05SYtsX277XEmfkLS+B32cxvbM6sCJbM+U9BH131DU6yXdXD2+WdKTPezll/TLMN6NhhlXj9+7ng9/HhFdv0laobEj8q9I+mwvemjQ1xWSXqxu23vdm6SHNbZbd1xjxzZukXShpI2Sdkr6nqTBPurta5K2StqisWDN71Fv12lsF32LpM3VbUWv37tCX1153/i6LJAEB+iAJAg7kARhB5Ig7EAShB1IgrADSRB2IIn/AzBpAPg08KfVAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mN7IXDwclnrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IgD3VclPlnuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "H1L3iDc2lnwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mjychvIElny6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UK_pmN-Vln1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP():\n",
        "  def __init__(self, epochs, num_input_nodes, hidden_layers, num_output_nodes, lr, optimizer, batch_size, activation_function = 'sigmoid', loss_type = 'cross_entropy', w_initial = 'glorot'):\n",
        "    self.epochs = epochs\n",
        "    self.lr = lr \n",
        "    self.optimizer = optimizer\n",
        "    self.optimizer.lr = self.lr      \n",
        "    self.batch_size = batch_size\n",
        "    self.num_input_nodes = num_input_nodes\n",
        "    self.hidden_layers = hidden_layers\n",
        "    self.num_output_nodes = num_output_nodes\n",
        "    self.loss_type = loss_type\n",
        "    #Activation function\n",
        "    self.activation_function = self.sigmoid\n",
        "    #parameter initialization\n",
        "    self.params = self.initialization(weight_initialisation = w_initial)\n",
        "\n",
        "  def sigmoid(self,x, derivative = False):\n",
        "    if derivative:\n",
        "      return self.sigmoid(x)*(1-self.sigmoid(x))\n",
        "    else:\n",
        "      return 1/(1 + np.exp(-x))\n",
        "\n",
        "  def one_hot(self,y):\n",
        "    v = np.zeros((self.num_output_nodes, len(y)))\n",
        "    for i,j in enumerate(y):\n",
        "      v[j,i] = 1\n",
        "    return v\n",
        "\n",
        "  def softmax(self,x,derivative = False):\n",
        "    if derivative:\n",
        "      return self.softmax(x)*(1- self.softmax(x))\n",
        "    else:\n",
        "      return (np.exp(x)/np.sum(np.exp(x), axis = 0))\n",
        "  def loss(self, y_pred, y_true):\n",
        "    if self.loss_type == 'cross_entropy':\n",
        "      return -1*np.mean(np.multiply(self.one_hot(y_true), np.log(y_pred))) # To Do: Optimize this using argmax\n",
        "\n",
        "  def initialization(self, weight_initialisation = 'random'):\n",
        "    w = []\n",
        "    b = []\n",
        "\n",
        "    if weight_initialisation == 'random':\n",
        "      w.append(np.random.randn(self.hidden_layers[0],self.num_input_nodes)*0.1)\n",
        "    elif weight_initialisation == 'Xavier':\n",
        "      w.append(np.random.randn(self.hidden_layers[0],self.num_input_nodes)*np.sqrt(2/(self.num_input_nodes+self.hidden_layers[0])))\n",
        "    b.append(np.zeros((self.hidden_layers[0], 1))) \n",
        "\n",
        "    for i in range(1,len(self.hidden_layers)):\n",
        "      if weight_initialisation == 'random':\n",
        "        w.append(np.random.randn(self.hidden_layers[i],self.hidden_layers[i-1])*0.1)\n",
        "      elif weight_initialisation == 'Xavier':\n",
        "        w.append(np.random.randn(self.hidden_layers[i],self.hidden_layers[i-1])*np.sqrt(2/(self.hidden_layers[i-1]+self.hidden_layers[i])))\n",
        "      b.append(np.zeros((self.hidden_layers[i], 1)))\n",
        "\n",
        "    if weight_initialisation == 'random':\n",
        "      w.append(np.random.randn(self.num_output_nodes,self.hidden_layers[len(self.hidden_layers)-1])*0.1)\n",
        "    elif weight_initialisation == 'Xavier':\n",
        "      w.append(np.random.randn(self.num_output_nodes,self.hidden_layers[len(self.hidden_layers)-1])*np.sqrt(2/(self.hidden_layers[len(self.hidden_layers)-1] + self.num_output_nodes)))\n",
        "    b.append(np.zeros((self.num_output_nodes, 1)))\n",
        "\n",
        "    return {'w':w, 'b':b}\n",
        "\n",
        "  def feed_forward(self, x):\n",
        "    #a=wh+b\n",
        "    #h=activation function(a)\n",
        "    self.a = []\n",
        "    self.h = []\n",
        "    self.h.append(x)\n",
        "\n",
        "    for i in range(0, len(self.hidden_layers)):\n",
        "      self.a.append(np.dot(self.params['w'][i], self.h[-1])+ self.params['b'][i])\n",
        "      self.h.append(self.activation_function(self.a[i]))\n",
        "\n",
        "    self.a.append(np.dot(self.params['w'][-1],self.h[-1])+self.params['b'][-1])\n",
        "\n",
        "    y_hat = self.softmax(self.a[-1])\n",
        "    return y_hat\n",
        "\n",
        "  def back_propagation(self, y_hat, y_true):\n",
        "    self.da = [0]* len(self.a)\n",
        "    self.dh = [0]* len(self.h)-1\n",
        "    self.dw = [0]* len(self.params['w'])\n",
        "    self.db = [0]* len(self.params['b'])\n",
        "\n",
        "    self.da[-1] = -1*(y_true - y_hat)\n",
        "\n",
        "    for i in range(len(self.params['w'])-1, 0, -1):\n",
        "      self.dw[i] = np.dot(self.da[i], self.h[i].T) \n",
        "      self.db[i] = self.da[i]\n",
        "\n",
        "      self.dh[i-1] = np.dot(self.params['w'][i].T, self.da[i])\n",
        "      self.da[i-1] = np.multiply(self.dh[i-1], self.activation_function(self.da[i-1], derivative=True))\n",
        "    self.dw[0] = self.da[0], self.dh[0].T\n",
        "    self.db[0] = self.da[0]\n",
        "\n",
        "    return self.dw, self.db\n"
      ],
      "metadata": {
        "id": "gUB162oqqzdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lv36qekrvPUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  def feed_forward(self, x):\n",
        "    #a=wh+b\n",
        "    #h=activation function(a)\n",
        "    self.a = []\n",
        "    self.h = []\n",
        "    self.h.append(x)\n",
        "\n",
        "    for i in range(0, len(self.hidden_layers)):\n",
        "      self.a.append(np.dot(self.params['w'][i], self.h[-1])+ self.params['b'][i])\n",
        "      self.h.append(self.activation_function(self.a[i]))\n",
        "\n",
        "    self.a.append(np.dot(self.params['w'][-1],self.h[-1])+self.params['b'][-1])\n",
        "\n",
        "    y_hat = self.softmax(self.a[-1])\n",
        "    return y_hat"
      ],
      "metadata": {
        "id": "shD1lxaXvu7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iDtiBAPvu94",
        "outputId": "5eb9a28f-0ace-4bd1-e032-8a82d4813a8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 784)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAx7xtc6w0VB",
        "outputId": "000e5e4b-a968-450d-9bae-037a5ad246d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 60000)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepNeuralNetwork():\n",
        "  def __init__(self,epochs,num_input_nodes, hidden_layers,num_output_nodes, lr, weight_decay, optimizer, batch_size, w_initial, activation_function = 'sigmoid', loss_type = 'cross_entropy'):\n",
        "    self.epochs = epochs\n",
        "    self.lr = lr \n",
        "    self.weight_decay = weight_decay\n",
        "    self.optimizer = optimizer\n",
        "    self.optimizer.lr = self.lr      \n",
        "    self.batch_size = batch_size\n",
        "    self.num_input_nodes = num_input_nodes\n",
        "    self.hidden_layers = hidden_layers\n",
        "    self.num_output_nodes = num_output_nodes\n",
        "    self.loss_type = loss_type\n",
        "    #Activation function\n",
        "    self.activation_function = self.activation(activation_function)\n",
        "\n",
        "    #parameter initialization\n",
        "    self.params = self.initialization(weight_initialisation = w_initial)\n",
        "\n",
        "\n",
        "  def initialization(self, weight_initialisation = 'random'):\n",
        "    w = []\n",
        "    b = []\n",
        "\n",
        "    if weight_initialisation == 'random':\n",
        "      w.append(np.random.randn(self.hidden_layers[0],self.num_input_nodes)*0.1)\n",
        "    elif weight_initialisation == 'Xavier':\n",
        "      w.append(np.random.randn(self.hidden_layers[0],self.num_input_nodes)*np.sqrt(2/(self.num_input_nodes+self.hidden_layers[0])))\n",
        "    b.append(np.zeros((self.hidden_layers[0], 1)))  #np.random.randn(self.hidden_layers[0],1)*0.01\n",
        "\n",
        "    for i in range(1,len(self.hidden_layers)):\n",
        "      if weight_initialisation == 'random':\n",
        "        w.append(np.random.randn(self.hidden_layers[i],self.hidden_layers[i-1])*0.1)\n",
        "      elif weight_initialisation == 'Xavier':\n",
        "        w.append(np.random.randn(self.hidden_layers[i],self.hidden_layers[i-1])*np.sqrt(2/(self.hidden_layers[i-1]+self.hidden_layers[i])))\n",
        "      #b.append(np.random.randn(self.hidden_layers[i],1)*0.01)\n",
        "      b.append(np.zeros((self.hidden_layers[i], 1)))\n",
        "\n",
        "    if weight_initialisation == 'random':\n",
        "      w.append(np.random.randn(self.num_output_nodes,self.hidden_layers[len(self.hidden_layers)-1])*0.1)\n",
        "    elif weight_initialisation == 'Xavier':\n",
        "      w.append(np.random.randn(self.num_output_nodes,self.hidden_layers[len(self.hidden_layers)-1])*np.sqrt(2/(self.hidden_layers[len(self.hidden_layers)-1] + self.num_output_nodes)))\n",
        "    #b.append(np.random.randn(self.num_output_nodes,1)*0.01)\n",
        "    b.append(np.zeros((self.num_output_nodes, 1)))\n",
        "\n",
        "    return {'w':w, 'b':b}\n",
        "\n",
        "  def activation(self, activation_function):\n",
        "\n",
        "    if activation_function == 'sigmoid':\n",
        "      return self.sigmoid\n",
        "    if activation_function == 'tanh':\n",
        "      return self.tanh\n",
        "    if activation_function == 'ReLU':\n",
        "      return self.relu\n",
        "\n",
        "  def sigmoid(self,x, derivative = False):\n",
        "    if derivative:\n",
        "      return (1/(1 + np.exp(-x)))*(1-(1/(1 + np.exp(-x))))#self.sigmoid(x)*(1-self.sigmoid(x))\n",
        "    else:\n",
        "      return 1/(1 + np.exp(-x))  \n",
        "\n",
        "  def tanh(self, x, derivative = False):\n",
        "    if derivative:\n",
        "      return 1 - self.tanh(x)**2\n",
        "    return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
        "\n",
        "  def relu(self, x, derivative = False):\n",
        "    if derivative:\n",
        "      return (x>0)*1 \n",
        "    return x*(x>0)\n",
        "\n",
        "  def softmax(self,x,derivative = False):\n",
        "    if derivative:\n",
        "      return (np.exp(x)/np.sum(np.exp(x), axis = 0))*(1- (np.exp(x)/np.sum(np.exp(x), axis = 0))) #self.softmax(x)*(1- self.softmax(x))\n",
        "    else:\n",
        "      return (np.exp(x)/np.sum(np.exp(x), axis = 0))\n",
        "\n",
        "  def one_hot(self,y):\n",
        "    v = np.zeros((self.num_output_nodes, len(y)))\n",
        "    for i,j in enumerate(y):\n",
        "      v[j,i] = 1\n",
        "    return v\n",
        "    \n",
        "\n",
        "  def compute_accuracy(self, x_val, y_val):\n",
        "\n",
        "    from sklearn.metrics import accuracy_score\n",
        "\n",
        "    x_val = x_val.reshape(self.num_input_nodes, -1)\n",
        "\n",
        "    out = self.feed_forward(x_val)\n",
        "    pred = np.argmax(out, axis = 0)\n",
        "\n",
        "    acc=accuracy_score(y_val, pred, normalize=True, sample_weight=None)\n",
        "\n",
        "\n",
        "    return np.mean(pred == y_val)*100\n",
        "\n",
        "\n",
        "\n",
        "  def loss(self, y_pred, y_true):\n",
        "    if self.loss_type == 'cross_entropy':\n",
        "      return -1*np.sum(np.multiply(self.one_hot(y_true), np.log(y_pred)))\n",
        "    if self.loss_type == 'mean_square_error':\n",
        "      return np.mean((self.one_hot(y_true) - y_pred) ** 2) #np.sum((self.one_hot(y_true) - y_pred) ** 2)*0.5\n",
        "\n",
        "  def feed_forward(self, x):\n",
        "    #a=wh+b\n",
        "    #h=activation function(a)\n",
        "    self.a = []\n",
        "    self.h = []\n",
        "    self.a.append(np.dot(self.params['w'][0], x) + self.params['b'][0])\n",
        "    self.h.append(self.activation_function(self.a[0]))\n",
        "\n",
        "    for i in range(1, len(self.hidden_layers)):\n",
        "      self.a.append(np.dot(self.params['w'][i], self.h[i-1])+ self.params['b'][i])\n",
        "      self.h.append(self.activation_function(self.a[i]))\n",
        "\n",
        "    self.a.append(np.dot(self.params['w'][len(self.hidden_layers)],self.h[len(self.hidden_layers)-1])+self.params['b'][len(self.hidden_layers)])\n",
        "\n",
        "    out = self.softmax(self.a[len(self.hidden_layers)])\n",
        "    return out\n",
        "    \"\"\"\n",
        "  def back_propagation(self,x, y_train,y_train_prob):\n",
        "    self.ga= [0]*(len(self.a))\n",
        "    self.gh= [0]*(len(self.h)) \n",
        "    self.gW= [0]*(len(self.params['w']))\n",
        "    self.gB= [0]*(len(self.params['b']))\n",
        "\n",
        "    if self.loss_type == 'cross_entropy':\n",
        "      self.ga[len(self.hidden_layers)]= -1*(self.one_hot(y_train)-y_train_prob)\n",
        "\n",
        "    if self.loss_type== 'mean_square_error': \n",
        "      self.ga[len(self.hidden_layers)]= np.multiply(2 *(y_train_prob -self.one_hot(y_train)), np.multiply(y_train_prob, (1 - y_train_prob)))\n",
        "      \n",
        "  \n",
        "      #(self.one_hot(y_train)-y_train_prob)*self.one_hot(y_train)*(1 - self.one_hot(y_train))  ## Check this Y-->y_train_prob,Y_train_batch->y_train\n",
        "\n",
        "    for i in range(len(self.hidden_layers), 0,-1):\n",
        "      self.gW[i] = np.outer(self.ga[i], self.h[i-1])   ###\n",
        "      self.gB[i] = self.ga[i]\n",
        "\n",
        "      self.gh[i-1] = np.matmul(self.params['w'][i].T, self.ga[i])\n",
        "      self.ga[i-1] = np.multiply(self.gh[i-1], self.activation_function(self.a[i-1], derivative = True))  ### Check this\n",
        "    \n",
        "    self.gW[0] = np.dot(self.ga[0], x.T)\n",
        "    self.gB[0] = np.dot(self.ga[0], np.ones((self.batch_size, 1)))\n",
        "\n",
        "    return self.gW, self.gB\n",
        "    \"\"\"\n",
        "########\n",
        "    \n",
        "  def back_propagation(self,x, y_train,y_train_prob):\n",
        "    self.ga= [0]*(len(self.a))\n",
        "    self.gh= [0]*(len(self.h)) \n",
        "    self.gW= [0]*(len(self.params['w']))\n",
        "    self.gB= [0]*(len(self.params['b']))\n",
        "    #num_layers=len(self.hidden_layers)+1\n",
        "    num_layers=len(self.params['w'])+1\n",
        "    gradients_weights = []\n",
        "    gradients_biases = []\n",
        "    if self.loss_type == 'cross_entropy':\n",
        "      self.ga[num_layers-1]= -1*(self.one_hot(y_train)-y_train_prob)\n",
        "\n",
        "    if self.loss_type== 'mean_square_error': \n",
        "      self.ga[num_layers-1]= np.multiply(2 *(y_train_prob -self.one_hot(y_train)), np.multiply(y_train_prob, (1 - y_train_prob)))\n",
        "      \n",
        "  \n",
        "      #(self.one_hot(y_train)-y_train_prob)*self.one_hot(y_train)*(1 - self.one_hot(y_train))  ## Check this Y-->y_train_prob,Y_train_batch->y_train\n",
        "\n",
        "    for i in range(num_layers -2, -1,-1):\n",
        "      self.gW[i+1] = np.outer(self.ga[i+1], self.h[i])   ###\n",
        "      self.gB[i+1] = self.ga[i+1]\n",
        "      gradients_weights.append(self.gW[i+1])\n",
        "      gradients_biases.append(self.gB[i+1])\n",
        "      if i!=0:\n",
        "        self.gh[i] = np.matmul(self.params['w'][i+1].T, self.ga[i+1])\n",
        "        self.ga[i] = np.multiply(self.gh[i], self.activation_function(self.a[i], derivative = True))  ### Check this\n",
        "      if i==0:\n",
        "        self.gh[i] = np.matmul(self.params['w'][i+1].T, self.ga[i+1])\n",
        "        self.ga[i] = np.multiply(self.gh[i], self.a[i])  ### Check this\n",
        "  \n",
        "    return gradients_weights, gradients_biases\n",
        "    \n",
        "  \"\"\"\n",
        "  def initgrads(N=3,Nunits=[32,64,16]):\n",
        "    dh,da,dw,db=[],[],[],[]\n",
        "    dw.append(np.zeros((Nunits[0],784)))\n",
        "    db.append(np.zeros(Nunits[0]))\n",
        "    da.append(np.zeros(Nunits[0]))\n",
        "    dh.append(np.zeros(Nunits[0]))\n",
        "    for i in range(1,N):\n",
        "      dw.append(np.zeros((Nunits[i],Nunits[i-1])))\n",
        "      db.append(np.zeros(Nunits[i]))\n",
        "      da.append(np.zeros(Nunits[i]))\n",
        "      dh.append(np.zeros(Nunits[i]))\n",
        "    dw.append(np.zeros((10,Nunits[-1])))\n",
        "    db.append(np.zeros(10))\n",
        "    da.append(np.zeros(10))\n",
        "    dh.append(np.zeros(10))\n",
        "    return dw,db,da,dh    \n",
        "  \n",
        "  def back_propagation(self,ip,ypr,ty,a_i,h_i,W,B,N,nrl,activation,Loss):\n",
        "    k=len(ypr)\n",
        "    dw,db,da,dh=initgrads(N=3,nrl=[32,64,16])\n",
        "    if (Loss==\"cross_entropy\"):\n",
        "      dh[-1]=[-(t/ypr) if t==1 else 0 for t in ty ]\n",
        "      da[-1]=ypr-ty\n",
        "    elif (Loss==\"mean_square_error\"):\n",
        "      dh[-1]=(ypr-ty)\n",
        "      da[-1]=dh[-1]*(ypr-ypr**2)\n",
        "\n",
        "    db[-1]=da[-1] \n",
        "    dw[-1]=np.dot((da[-1][:,np.newaxis]),(h_i[-2][:,np.newaxis]).T)\n",
        " \n",
        "    for i in range(N-1,-1,-1):\n",
        "      dh[i]=np.squeeze(np.dot(W[i+1].T,da[i+1]))\n",
        "\n",
        "      da[i]=dh[i]*self.activation_function(self.a_i[i], derivative = True)\n",
        "  \n",
        "      db[i]=np.copy(da[i])\n",
        "      if (i==0):\n",
        "        dw[i]=np.dot(da[i][:,np.newaxis],ip[:,np.newaxis].T)\n",
        "      else:\n",
        "        dw[i]=np.dot(da[i][:,np.newaxis],h_i[i-1][:,np.newaxis].T)\n",
        "    return dw,db\n",
        "    \"\"\"\n",
        "\n",
        "  def train(self, x_train, y_train, x_val, y_val):\n",
        "    N = x_train.shape[0]\n",
        "    n_batches = int(np.floor(N / self.batch_size))\n",
        "    for epoch in range(0, self.epochs):\n",
        "\n",
        "      #data shuffle\n",
        "      # shuffler = np.random.permutation(len(x_train))\n",
        "      # x_train = x_train[shuffler]\n",
        "      # y_train = y_train[shuffler]\n",
        "\n",
        "      l = 0\n",
        "      for batch in tqdm(range(0, n_batches)):\n",
        "        x = x_train[batch*self.batch_size:self.batch_size+batch*self.batch_size].reshape(self.num_input_nodes,self.batch_size)\n",
        "        y = y_train[batch*self.batch_size:self.batch_size+batch*self.batch_size]\n",
        "        out = self.feed_forward(x)\n",
        "        \"\"\"    \n",
        "        yhot=self.one_hot(y)\n",
        "        print(x[:,0].shape)  #ftrx[i]\n",
        "        print(yhot[:,0].shape) #ftry[i]\n",
        "         \"\"\"\n",
        "        # gw,gb=Back_Prop(ftrx[i],pred_y,ftry[i],ai,hi,W,B,N,nrl,activation,Ls_fun)\n",
        "        gW, gB = self.back_propagation(x, y, out)\n",
        "        self.params = self.optimizer.update(self.params, gW,gB)\n",
        "        l += self.loss(out, y)\n",
        "       \n",
        "      try:\n",
        "        x = x_train[-1*N%n_batches:].reshape(self.num_input_nodes,N%n_batches)\n",
        "        y = y_train[-1*N%n_batches:]\n",
        "        out = self.feed_forward(x)\n",
        "        gW, gB = self.back_propagation(x, y, out)\n",
        "        self.params = self.optimizer.update(self.params, gW,gB)\n",
        "        l += self.loss(out, y)\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "      print(f\"Epoch: {epoch}, loss: {l/N}\")\n",
        "      print(f\"Accuracy: {self.compute_accuracy(x_val, y_val)}\")\n"
      ],
      "metadata": {
        "id": "QrAMKtQ6q0KB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}